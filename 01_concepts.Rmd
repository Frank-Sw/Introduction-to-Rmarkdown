# Concepts

To use R Markdown effectively, it helps to know why you'd even want to. Let's talk about some ideas, all of which are related, and which will give you some sense of the goals of effective document generation, and why this approach is superior to others you might try.

Some terminology you might have come across:

- Reproducible research
- Replicable science
- Reproducible data analysis
- Dynamic data analysis
- Dynamic report generation
- Literate programming

Each of these may mean slightly different things depending on the context and background of the person using them, so one should take care to note precisely what is meant.  We'll examine a couple of these concepts, or at least my particular version of them.

## Literate Programming

<span class="emph">Literate programming</span>, or in the context of research, literate statistical programming, is actually an [old idea](http://www.literateprogramming.com/knuthweb.pdf) at this point.  

> I believe that the time is ripe for significantly better documentation of programs, and that we can best achieve this by considering programs to be works of literature. <br> ~ Donald Knuth (1984)

...

The interweaving of code and text is something you do already in normal scripting.  Comments in code are not only useful, they are practically required.  But in a program script, almost all the emphasis is on the code.  With literate programming, we focus on the text, and the code exists to help facilitate our ability to tell a (data-driven) story.

In the early days the idea was largely to communicate the idea of the computer program itself. Now, at least in the context we'll be discussing, our usage of literate programming is generate results that tell the story in a completely human-oriented fashion, possibly without any reference to the code at all.  However, the document, in whatever format, does not exist independently of the code, and cannot be generated without it.

Consider the following example. This code shows how to do an unordered list in markdown using two different methods.  Either a `-` or a `*` will denote a list item.

```markdown
- item 1
- item 2

* item 3
* item 4
```

In the previous we have a statement explaining the code, followed by the code itself. We actually don't need a code comment, because the text explains the code in everyday language.  This is a simple example but gets at the essence of the approach.  In the document you're reading right now, there is a delineation for the code itself, but this isn't visible. However it's clear what the code part is and what the text part is.  

The following table shows the results of a regression analysis.

```{r lm_demo,echo=FALSE}
summary(lm(mpg ~ wt, mtcars)) %>% 
  pander(round=3)
```

So imagine a paper in which the previous text content explains the results while the analysis code resides right where the text is.  You didn't see the code, but you saw some nicely formatted results. I personally didn't format anything however, those are using default settings.  Here is the underlying code.


```{r lm_demo2, eval=FALSE}
lm(mpg ~ wt, mtcars) %>% 
  summary() %>% 
  pander()
```

Here we see the code, but it isn't evaluated, because the goal of the text here is not the result but to understand the code.  Nothing is copied and pasted, the code and text both reside in the same document.

The idea of literate programming, i.e. creating human-understandable programs, can extend beyond reports or slides that you might put together for an analysis, and in fact be used for any setting in which you have to write code at all.


## Replicability & reproducible research

The ideas of <span class="emph">replicability</span> and <span class="emph">reproducible research</span> are hot topics in various disciplines of late.  To begin, neither is precisely defined, and depending on the definition one selects, possibly not very likely or even impossible.  They are more ideals to strive for, or goals for certain aspects of the research process.  For example, nothing is exactly replicable, if only because time will have passed, and with it some things will have changed about the process, the people involved, the data collection, the analytical tools, etc., since the initial research was conducted.  However, we can replicate some things, possibly even exactly, and thus make the results reproducible.

Our focus will be on the data analysis side.  Let's start with the following scenario. Various versions of a dataset used in analysis, and after several iterations, `finaldata7` is now spread across the computers of the faculty advisor, two graduate students and one undergrad. Two of those finaldata7 data sets, specifically named `finaldata7a` and `finaldata7b`, are slightly different from the other two and each other.  The undergraduate, who helped with the processing of finaldata2 through finaldata6 has graduated and no longer resides in the same state, nor likely cares any more about the project.  Some of the data processing was done with menus in a software package that shall not be named.

The script that did the final analysis, called `model.Results.C`, calls the data using a directory location which no longer exists (and refers only to `finaldata7`).  It does several more data processing steps, but has no comments that would indicate why any of them are being done.  Some of the variables are named things like PDQ and V3, but there is no document that would say what those mean.

When writing their document in Microsoft Word, all the values from the analyses were copied and pasted into the tables and text[^intheback].  The variable names in the document have no exact match to any of the names in any of the finaldatas.  Furthermore, no reference was provided in the text what software or specific packages were used for the analysis.  

And now, several months later, the journal reviewers have made their comments on the paper, and it's time to dive back into the analysis.  Now, what do you think the odds are that this research group could even reproduce the main analysis of the paper?

Sadly, up until a couple years ago this was not uncommon, and even certain issues described are still very common.  Such an approach is essentially the antithesis of replicability and reproducible research.  Anything that was only done with menus cannot be replicated, and without sufficient documentation it's not clear what was done even when there is potentially reproducible code.  The naming of files, variables and other objects was done poorly, so it will take unnecessary effort to figure out what was done and when.  And even after most things get squared away, there is still a chance the numbers won't match what was in the paper anyway.

While certain tools like Box, Dropbox, Google drive etc. have perhaps helped some, people generally don't use them for version control, and they are not geared toward academic research specifically.  However, using proper naming procedures, the approach of literate programming


  
## Dynamic data analysis & report generation


## Comparison to alternatives

MS Word

$\LaTeX$



[^intheback]: And since this journal still thinks it's 1990, all the tables had to be at the end of the document, so they aren't even near the text which refers to them.